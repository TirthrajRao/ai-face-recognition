arguments: train_tripleloss_2.py
--------------------
tensorflow version: 1.13.2
--------------------
git hash: b'c7dc9188846b3781e1ed512dda90627143289f8c'
--------------------
b'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\nindex 79f793e..2ed2250 100644\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\ndiff --git a/faceNet-yolo/align-dataset-mtcnn.py b/faceNet-yolo/align-dataset-mtcnn.py\nindex dd5cbf1..aedc556 100644\n--- a/faceNet-yolo/align-dataset-mtcnn.py\n+++ b/faceNet-yolo/align-dataset-mtcnn.py\n@@ -32,8 +32,8 @@ import os\n import argparse\n import tensorflow as tf\n import numpy as np\n-# import facenet\n-import facenet.src.facenet as facenet\n+import facenet\n+# import facenet.src.facenet as facenet\n import detect_face\n import random\n from time import sleep\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_03860.txt b/faceNet-yolo/aligned_faces/bounding_boxes_03860.txt\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_34040.txt b/faceNet-yolo/aligned_faces/bounding_boxes_34040.txt\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_35766.txt b/faceNet-yolo/aligned_faces/bounding_boxes_35766.txt\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_59326.txt b/faceNet-yolo/aligned_faces/bounding_boxes_59326.txt\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_73169.txt b/faceNet-yolo/aligned_faces/bounding_boxes_73169.txt\ndeleted file mode 100644\nindex 5e22ef6..0000000\n--- a/faceNet-yolo/aligned_faces/bounding_boxes_73169.txt\n+++ /dev/null\n@@ -1,3 +0,0 @@\n-./aligned_faces/s1/pr_train_1.png 171 362 561 874\n-./aligned_faces/s2/ishita1.png 275 480 518 825\n-./aligned_faces/s3/papa2.png 222 289 561 722\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_79551.txt b/faceNet-yolo/aligned_faces/bounding_boxes_79551.txt\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/faceNet-yolo/aligned_faces/revision_info.txt b/faceNet-yolo/aligned_faces/revision_info.txt\nindex b089a22..4d40484 100644\n--- a/faceNet-yolo/aligned_faces/revision_info.txt\n+++ b/faceNet-yolo/aligned_faces/revision_info.txt\n@@ -1,7 +1,7 @@\n arguments: align-dataset-mtcnn.py ./unaligned_faces ./aligned_faces\n --------------------\n-tensorflow version: 2.1.0\n+tensorflow version: 1.13.2\n --------------------\n-git hash: b\'f6c9d2a50ce53c4430be0a49d4b7457cf10345fb\'\n+git hash: b\'c7dc9188846b3781e1ed512dda90627143289f8c\'\n --------------------\n-b\'diff --git a/cv2/raam1.jpeg b/cv2/raam1.jpeg\\ndeleted file mode 100644\\nindex 78c6dd2..0000000\\nBinary files a/cv2/raam1.jpeg and /dev/null differ\\ndiff --git a/cv2/raam2.jpeg b/cv2/raam2.jpeg\\ndeleted file mode 100644\\nindex 78c6dd2..0000000\\nBinary files a/cv2/raam2.jpeg and /dev/null differ\\ndiff --git a/cv2/train/s1/pr_train_1.jpeg b/cv2/train/s1/pr_train_1.jpeg\\ndeleted file mode 100644\\nindex 4b3de78..0000000\\nBinary files a/cv2/train/s1/pr_train_1.jpeg and /dev/null differ\\ndiff --git a/cv2/train/s2/ishita1.jpeg b/cv2/train/s2/ishita1.jpeg\\ndeleted file mode 100644\\nindex 2cbb3e3..0000000\\nBinary files a/cv2/train/s2/ishita1.jpeg and /dev/null differ\\ndiff --git a/cv2/train/s3/papa2.jpeg b/cv2/train/s3/papa2.jpeg\\ndeleted file mode 100644\\nindex 70e65b2..0000000\\nBinary files a/cv2/train/s3/papa2.jpeg and /dev/null differ\\ndiff --git a/go/go_1.go b/go/go_1.go\\nindex 29019b5..27e4f7d 100644\\n--- a/go/go_1.go\\n+++ b/go/go_1.go\\n@@ -55,8 +55,8 @@ func main() {\\n \\n \\t// Prepare the testing data\\n \\tpaths = nil\\n-\\tpaths = append(paths, "./pr_test_6.jpg")\\n-\\tpaths = append(paths, "./ishita_test_2.jpg")\\n+\\tpaths = append(paths, "./pr_test_7.jpg")\\n+\\tpaths = append(paths, "./ishita_test_8.jpg")\\n \\tpaths = append(paths, "./papa_test_1.jpg")\\n \\t// paths = append(paths, "./papa2.jpg")\'\n\\ No newline at end of file\n+b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 79f793e..2ed2250 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/align-dataset-mtcnn.py b/faceNet-yolo/align-dataset-mtcnn.py\\nindex dd5cbf1..aedc556 100644\\n--- a/faceNet-yolo/align-dataset-mtcnn.py\\n+++ b/faceNet-yolo/align-dataset-mtcnn.py\\n@@ -32,8 +32,8 @@ import os\\n import argparse\\n import tensorflow as tf\\n import numpy as np\\n-# import facenet\\n-import facenet.src.facenet as facenet\\n+import facenet\\n+# import facenet.src.facenet as facenet\\n import detect_face\\n import random\\n from time import sleep\\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_03860.txt b/faceNet-yolo/aligned_faces/bounding_boxes_03860.txt\\ndeleted file mode 100644\\nindex e69de29..0000000\\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_34040.txt b/faceNet-yolo/aligned_faces/bounding_boxes_34040.txt\\ndeleted file mode 100644\\nindex e69de29..0000000\\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_35766.txt b/faceNet-yolo/aligned_faces/bounding_boxes_35766.txt\\ndeleted file mode 100644\\nindex e69de29..0000000\\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_59326.txt b/faceNet-yolo/aligned_faces/bounding_boxes_59326.txt\\ndeleted file mode 100644\\nindex e69de29..0000000\\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_73169.txt b/faceNet-yolo/aligned_faces/bounding_boxes_73169.txt\\ndeleted file mode 100644\\nindex 5e22ef6..0000000\\n--- a/faceNet-yolo/aligned_faces/bounding_boxes_73169.txt\\n+++ /dev/null\\n@@ -1,3 +0,0 @@\\n-./aligned_faces/s1/pr_train_1.png 171 362 561 874\\n-./aligned_faces/s2/ishita1.png 275 480 518 825\\n-./aligned_faces/s3/papa2.png 222 289 561 722\\ndiff --git a/faceNet-yolo/aligned_faces/bounding_boxes_79551.txt b/faceNet-yolo/aligned_faces/bounding_boxes_79551.txt\\ndeleted file mode 100644\\nindex e69de29..0000000\\ndiff --git a/faceNet-yolo/aligned_faces/revision_info.txt b/faceNet-yolo/aligned_faces/revision_info.txt\\ndeleted file mode 100644\\nindex b089a22..0000000\\n--- a/faceNet-yolo/aligned_faces/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: align-dataset-mtcnn.py ./unaligned_faces ./aligned_faces\\n---------------------\\n-tensorflow version: 2.1.0\\n---------------------\\n-git hash: b\\\'f6c9d2a50ce53c4430be0a49d4b7457cf10345fb\\\'\\n---------------------\\n-b\\\'diff --git a/cv2/raam1.jpeg b/cv2/raam1.jpeg\\\\ndeleted file mode 100644\\\\nindex 78c6dd2..0000000\\\\nBinary files a/cv2/raam1.jpeg and /dev/null differ\\\\ndiff --git a/cv2/raam2.jpeg b/cv2/raam2.jpeg\\\\ndeleted file mode 100644\\\\nindex 78c6dd2..0000000\\\\nBinary files a/cv2/raam2.jpeg and /dev/null differ\\\\ndiff --git a/cv2/train/s1/pr_train_1.jpeg b/cv2/train/s1/pr_train_1.jpeg\\\\ndeleted file mode 100644\\\\nindex 4b3de78..0000000\\\\nBinary files a/cv2/train/s1/pr_train_1.jpeg and /dev/null differ\\\\ndiff --git a/cv2/train/s2/ishita1.jpeg b/cv2/train/s2/ishita1.jpeg\\\\ndeleted file mode 100644\\\\nindex 2cbb3e3..0000000\\\\nBinary files a/cv2/train/s2/ishita1.jpeg and /dev/null differ\\\\ndiff --git a/cv2/train/s3/papa2.jpeg b/cv2/train/s3/papa2.jpeg\\\\ndeleted file mode 100644\\\\nindex 70e65b2..0000000\\\\nBinary files a/cv2/train/s3/papa2.jpeg and /dev/null differ\\\\ndiff --git a/go/go_1.go b/go/go_1.go\\\\nindex 29019b5..27e4f7d 100644\\\\n--- a/go/go_1.go\\\\n+++ b/go/go_1.go\\\\n@@ -55,8 +55,8 @@ func main() {\\\\n \\\\n \\\\t// Prepare the testing data\\\\n \\\\tpaths = nil\\\\n-\\\\tpaths = append(paths, "./pr_test_6.jpg")\\\\n-\\\\tpaths = append(paths, "./ishita_test_2.jpg")\\\\n+\\\\tpaths = append(paths, "./pr_test_7.jpg")\\\\n+\\\\tpaths = append(paths, "./ishita_test_8.jpg")\\\\n \\\\tpaths = append(paths, "./papa_test_1.jpg")\\\\n \\\\t// paths = append(paths, "./papa2.jpg")\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/aligned_faces/s1/pr_train_1.png b/faceNet-yolo/aligned_faces/s1/pr_train_1.png\\ndeleted file mode 100644\\nindex ea58b77..0000000\\nBinary files a/faceNet-yolo/aligned_faces/s1/pr_train_1.png and /dev/null differ\\ndiff --git a/faceNet-yolo/aligned_faces/s2/ishita1.png b/faceNet-yolo/aligned_faces/s2/ishita1.png\\ndeleted file mode 100644\\nindex fc33eb6..0000000\\nBinary files a/faceNet-yolo/aligned_faces/s2/ishita1.png and /dev/null differ\\ndiff --git a/faceNet-yolo/aligned_faces/s3/papa2.png b/faceNet-yolo/aligned_faces/s3/papa2.png\\ndeleted file mode 100644\\nindex d1c2694..0000000\\nBinary files a/faceNet-yolo/aligned_faces/s3/papa2.png and /dev/null differ\\ndiff --git a/faceNet-yolo/facenet.py b/faceNet-yolo/facenet.py\\nindex fc7da25..c2b2b44 100644\\n--- a/faceNet-yolo/facenet.py\\n+++ b/faceNet-yolo/facenet.py\\n@@ -363,6 +363,7 @@ def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\n     return train_set, test_set\\n \\n def load_model(model, input_map=None):\\n+    print(" ==============> ", model)\\n     # Check if the model is a model directory (containing a metagraph and a checkpoint file)\\n     #  or if it is a protobuf file with a frozen graph\\n     model_exp = os.path.expanduser(model)\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-164246/arguments.txt b/faceNet-yolo/logs/facenet/20200413-164246/arguments.txt\\ndeleted file mode 100644\\nindex 4394f23..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-164246/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 15\\n-image_size: 160\\n-people_per_batch: 6\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-164246/events.out.tfevents.1586776388.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-164246/events.out.tfevents.1586776388.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 73b6ba9..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-164246/events.out.tfevents.1586776388.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-164246/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-164246/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-164246/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-165512/arguments.txt b/faceNet-yolo/logs/facenet/20200413-165512/arguments.txt\\ndeleted file mode 100644\\nindex 139f2e3..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-165512/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-165512/events.out.tfevents.1586777134.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-165512/events.out.tfevents.1586777134.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 114ee3b..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-165512/events.out.tfevents.1586777134.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-165512/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-165512/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-165512/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170019/arguments.txt b/faceNet-yolo/logs/facenet/20200413-170019/arguments.txt\\ndeleted file mode 100644\\nindex 6f47b3d..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-170019/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 1\\n-epoch_size: 1\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170019/events.out.tfevents.1586777441.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-170019/events.out.tfevents.1586777441.rao-B360M-D2V\\ndeleted file mode 100644\\nindex f8ac3b5..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-170019/events.out.tfevents.1586777441.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170019/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-170019/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-170019/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170139/arguments.txt b/faceNet-yolo/logs/facenet/20200413-170139/arguments.txt\\ndeleted file mode 100644\\nindex 6f47b3d..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-170139/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 1\\n-epoch_size: 1\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170139/events.out.tfevents.1586777534.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-170139/events.out.tfevents.1586777534.rao-B360M-D2V\\ndeleted file mode 100644\\nindex eaa431f..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-170139/events.out.tfevents.1586777534.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170139/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-170139/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-170139/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170533/arguments.txt b/faceNet-yolo/logs/facenet/20200413-170533/arguments.txt\\ndeleted file mode 100644\\nindex 6f47b3d..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-170533/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 1\\n-epoch_size: 1\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170533/events.out.tfevents.1586777788.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-170533/events.out.tfevents.1586777788.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 2ab7a3b..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-170533/events.out.tfevents.1586777788.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170533/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-170533/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-170533/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170838/arguments.txt b/faceNet-yolo/logs/facenet/20200413-170838/arguments.txt\\ndeleted file mode 100644\\nindex 6f47b3d..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-170838/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 1\\n-epoch_size: 1\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170838/events.out.tfevents.1586777970.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-170838/events.out.tfevents.1586777970.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 55e0a58..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-170838/events.out.tfevents.1586777970.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170838/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-170838/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-170838/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-171326/arguments.txt b/faceNet-yolo/logs/facenet/20200413-171326/arguments.txt\\ndeleted file mode 100644\\nindex bf8332a..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-171326/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./output\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 15\\n-image_size: 160\\n-people_per_batch: 6\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-171326/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-171326/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-171326/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-180801/arguments.txt b/faceNet-yolo/logs/facenet/20200413-180801/arguments.txt\\ndeleted file mode 100644\\nindex 6fb7f7a..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-180801/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 15\\n-image_size: 160\\n-people_per_batch: 6\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-180801/events.out.tfevents.1586781539.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-180801/events.out.tfevents.1586781539.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 4e7dc8a..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-180801/events.out.tfevents.1586781539.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-180801/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-180801/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-180801/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181225/arguments.txt b/faceNet-yolo/logs/facenet/20200413-181225/arguments.txt\\ndeleted file mode 100644\\nindex cf47e42..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-181225/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 15\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181225/events.out.tfevents.1586781768.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-181225/events.out.tfevents.1586781768.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 961837f..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-181225/events.out.tfevents.1586781768.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181225/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-181225/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-181225/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181822/arguments.txt b/faceNet-yolo/logs/facenet/20200413-181822/arguments.txt\\ndeleted file mode 100644\\nindex cf47e42..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-181822/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 15\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181822/events.out.tfevents.1586782124.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-181822/events.out.tfevents.1586782124.rao-B360M-D2V\\ndeleted file mode 100644\\nindex cdba8c7..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-181822/events.out.tfevents.1586782124.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181822/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-181822/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-181822/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184739/arguments.txt b/faceNet-yolo/logs/facenet/20200413-184739/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-184739/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184739/events.out.tfevents.1586783885.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-184739/events.out.tfevents.1586783885.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 3341c00..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-184739/events.out.tfevents.1586783885.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184739/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-184739/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-184739/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184938/arguments.txt b/faceNet-yolo/logs/facenet/20200413-184938/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-184938/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184938/events.out.tfevents.1586784005.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-184938/events.out.tfevents.1586784005.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 88d6220..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-184938/events.out.tfevents.1586784005.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184938/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-184938/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-184938/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-185114/arguments.txt b/faceNet-yolo/logs/facenet/20200413-185114/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-185114/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-185114/events.out.tfevents.1586784102.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-185114/events.out.tfevents.1586784102.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 2f7096a..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200413-185114/events.out.tfevents.1586784102.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200413-185114/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-185114/revision_info.txt\\ndeleted file mode 100644\\nindex f8acf7b..0000000\\n--- a/faceNet-yolo/logs/facenet/20200413-185114/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'7c7dadf82d042710349852385539f58303124612\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-085926/arguments.txt b/faceNet-yolo/logs/facenet/20200414-085926/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-085926/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-085926/events.out.tfevents.1586834990.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-085926/events.out.tfevents.1586834990.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 88c44bd..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-085926/events.out.tfevents.1586834990.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-085926/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-085926/revision_info.txt\\ndeleted file mode 100644\\nindex 7efc726..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-085926/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090615/arguments.txt b/faceNet-yolo/logs/facenet/20200414-090615/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-090615/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090615/events.out.tfevents.1586835399.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-090615/events.out.tfevents.1586835399.rao-B360M-D2V\\ndeleted file mode 100644\\nindex e1e64cf..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-090615/events.out.tfevents.1586835399.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090615/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-090615/revision_info.txt\\ndeleted file mode 100644\\nindex b4d80e3..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-090615/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..2d28df3 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -243,6 +243,7 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         # Perform training on the selected triplets\\\\n         nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\\\\n         triplet_paths = list(itertools.chain(*triplets))\\\\n+        print("triplet_paths =====> ", triplet_paths, len(triplet_paths))\\\\n         labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\\\\n         triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\\\\n         sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090756/arguments.txt b/faceNet-yolo/logs/facenet/20200414-090756/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-090756/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090756/events.out.tfevents.1586835499.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-090756/events.out.tfevents.1586835499.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 6ccfb65..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-090756/events.out.tfevents.1586835499.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090756/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-090756/revision_info.txt\\ndeleted file mode 100644\\nindex 94c4d2d..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-090756/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..97c8a9d 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -220,6 +220,8 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n         print("nrof_examples =========+>", nrof_examples)\\\\n+        print("triplet_paths =====> ", nrof_examples, len(nrof_examples))\\\\n+        \\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090854/arguments.txt b/faceNet-yolo/logs/facenet/20200414-090854/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-090854/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090854/events.out.tfevents.1586835556.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-090854/events.out.tfevents.1586835556.rao-B360M-D2V\\ndeleted file mode 100644\\nindex d567305..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-090854/events.out.tfevents.1586835556.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090854/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-090854/revision_info.txt\\ndeleted file mode 100644\\nindex 728dd00..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-090854/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..9547975 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -220,6 +220,8 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n         print("nrof_examples =========+>", nrof_examples)\\\\n+        print("triplet_paths =====> ", nrof_examples)\\\\n+\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091110/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091110/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091110/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091110/events.out.tfevents.1586835691.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091110/events.out.tfevents.1586835691.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 36ca54e..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-091110/events.out.tfevents.1586835691.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091110/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091110/revision_info.txt\\ndeleted file mode 100644\\nindex 2953a43..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091110/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..1470414 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -220,7 +220,8 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n         print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\\\n+        print("DONEEEE=========+>")\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091208/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091208/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091208/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091208/events.out.tfevents.1586835750.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091208/events.out.tfevents.1586835750.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 37b5194..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-091208/events.out.tfevents.1586835750.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091208/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091208/revision_info.txt\\ndeleted file mode 100644\\nindex a35b0d7..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091208/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..f74df72 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,8 +219,9 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\\\n+        print("DONEEEE=========+>")\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091502/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091502/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091502/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091502/events.out.tfevents.1586835924.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091502/events.out.tfevents.1586835924.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 80f853f..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-091502/events.out.tfevents.1586835924.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091502/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091502/revision_info.txt\\ndeleted file mode 100644\\nindex 834b914..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091502/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..ab0c4a3 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,8 +219,10 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\\\n+        labels_array = np.arange(nrof_examples).reshape(-1,3)\\\\n+        print("DONEEEE=========+>")\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091650/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091650/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091650/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091650/events.out.tfevents.1586836034.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091650/events.out.tfevents.1586836034.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 1110554..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-091650/events.out.tfevents.1586836034.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091650/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091650/revision_info.txt\\ndeleted file mode 100644\\nindex f04b848..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091650/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..ec0fa70 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,8 +219,10 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\\\n+        labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE=========+>")\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091826/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091826/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091826/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091826/events.out.tfevents.1586836130.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091826/events.out.tfevents.1586836130.rao-B360M-D2V\\ndeleted file mode 100644\\nindex a0fa18e..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-091826/events.out.tfevents.1586836130.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091826/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091826/revision_info.txt\\ndeleted file mode 100644\\nindex d8e2271..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-091826/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..27cb427 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\\\n+        labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n+        # image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array)\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092010/arguments.txt b/faceNet-yolo/logs/facenet/20200414-092010/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-092010/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092010/events.out.tfevents.1586836234.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-092010/events.out.tfevents.1586836234.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 207897d..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-092010/events.out.tfevents.1586836234.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092010/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-092010/revision_info.txt\\ndeleted file mode 100644\\nindex e3ed4f3..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-092010/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..a660837 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,10 +219,14 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\\\n+        labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n+        # image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n+        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array[0], labels_placeholder: labels_array[0]})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n         for i in range(nrof_batches):\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092815/arguments.txt b/faceNet-yolo/logs/facenet/20200414-092815/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-092815/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092815/events.out.tfevents.1586836719.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-092815/events.out.tfevents.1586836719.rao-B360M-D2V\\ndeleted file mode 100644\\nindex dbca194..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-092815/events.out.tfevents.1586836719.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092815/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-092815/revision_info.txt\\ndeleted file mode 100644\\nindex 3d41b8a..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-092815/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..d60aca4 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\\\n+        labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n+        # image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-093742/arguments.txt b/faceNet-yolo/logs/facenet/20200414-093742/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-093742/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-093742/events.out.tfevents.1586837286.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-093742/events.out.tfevents.1586837286.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 3b6249d..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-093742/events.out.tfevents.1586837286.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-093742/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-093742/revision_info.txt\\ndeleted file mode 100644\\nindex 1ed16ef..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-093742/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..d795492 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-094136/arguments.txt b/faceNet-yolo/logs/facenet/20200414-094136/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-094136/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-094136/events.out.tfevents.1586837520.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-094136/events.out.tfevents.1586837520.rao-B360M-D2V\\ndeleted file mode 100644\\nindex e413aef..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-094136/events.out.tfevents.1586837520.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-094136/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-094136/revision_info.txt\\ndeleted file mode 100644\\nindex 61bb5ac..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-094136/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..4f9c298 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,0))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095012/arguments.txt b/faceNet-yolo/logs/facenet/20200414-095012/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-095012/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095012/events.out.tfevents.1586838034.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-095012/events.out.tfevents.1586838034.rao-B360M-D2V\\ndeleted file mode 100644\\nindex e6a9fa9..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-095012/events.out.tfevents.1586838034.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095012/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-095012/revision_info.txt\\ndeleted file mode 100644\\nindex 3a805c3..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-095012/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..e67be7e 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,1))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n+        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,1))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095136/arguments.txt b/faceNet-yolo/logs/facenet/20200414-095136/arguments.txt\\ndeleted file mode 100644\\nindex 0da86de..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-095136/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 1\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095136/events.out.tfevents.1586838119.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-095136/events.out.tfevents.1586838119.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 6c98d53..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-095136/events.out.tfevents.1586838119.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095136/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-095136/revision_info.txt\\ndeleted file mode 100644\\nindex f2f0b54..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-095136/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..7fbd596 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-113857/arguments.txt b/faceNet-yolo/logs/facenet/20200414-113857/arguments.txt\\ndeleted file mode 100644\\nindex 4394f23..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-113857/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 15\\n-image_size: 160\\n-people_per_batch: 6\\n-images_per_person: 1\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-113857/events.out.tfevents.1586844559.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-113857/events.out.tfevents.1586844559.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 982aa82..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-113857/events.out.tfevents.1586844559.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-113857/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-113857/revision_info.txt\\ndeleted file mode 100644\\nindex 9cacfaf..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-113857/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..f677692 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,11 +444,11 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=15)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=6)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114001/arguments.txt b/faceNet-yolo/logs/facenet/20200414-114001/arguments.txt\\ndeleted file mode 100644\\nindex 1834628..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-114001/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 15\\n-image_size: 160\\n-people_per_batch: 6\\n-images_per_person: 11\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114001/events.out.tfevents.1586844623.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-114001/events.out.tfevents.1586844623.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 6a42709..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-114001/events.out.tfevents.1586844623.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114001/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-114001/revision_info.txt\\ndeleted file mode 100644\\nindex b31acd8..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-114001/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..a51e814 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=15)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=6)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114146/arguments.txt b/faceNet-yolo/logs/facenet/20200414-114146/arguments.txt\\ndeleted file mode 100644\\nindex 05a0eb8..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-114146/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 15\\n-image_size: 160\\n-people_per_batch: 11\\n-images_per_person: 11\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114146/events.out.tfevents.1586844728.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-114146/events.out.tfevents.1586844728.rao-B360M-D2V\\ndeleted file mode 100644\\nindex f173d44..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-114146/events.out.tfevents.1586844728.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114146/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-114146/revision_info.txt\\ndeleted file mode 100644\\nindex 995ccd5..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-114146/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..01410b9 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=15)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114742/arguments.txt b/faceNet-yolo/logs/facenet/20200414-114742/arguments.txt\\ndeleted file mode 100644\\nindex 3562145..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-114742/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 11\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114742/events.out.tfevents.1586845084.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-114742/events.out.tfevents.1586845084.rao-B360M-D2V\\ndeleted file mode 100644\\nindex dafb85c..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-114742/events.out.tfevents.1586845084.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114742/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-114742/revision_info.txt\\ndeleted file mode 100644\\nindex 828779a..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-114742/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..fcf7912 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115550/arguments.txt b/faceNet-yolo/logs/facenet/20200414-115550/arguments.txt\\ndeleted file mode 100644\\nindex 0a3ebff..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-115550/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 6\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115550/events.out.tfevents.1586845572.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-115550/events.out.tfevents.1586845572.rao-B360M-D2V\\ndeleted file mode 100644\\nindex a897ed7..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-115550/events.out.tfevents.1586845572.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115550/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-115550/revision_info.txt\\ndeleted file mode 100644\\nindex 9978d16..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-115550/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..8d26aea 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=6)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=10)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115629/arguments.txt b/faceNet-yolo/logs/facenet/20200414-115629/arguments.txt\\ndeleted file mode 100644\\nindex a748622..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-115629/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115629/events.out.tfevents.1586845611.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-115629/events.out.tfevents.1586845611.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 129aa31..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-115629/events.out.tfevents.1586845611.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115629/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-115629/revision_info.txt\\ndeleted file mode 100644\\nindex e0c6c55..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-115629/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..82d19b8 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=10)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115714/arguments.txt b/faceNet-yolo/logs/facenet/20200414-115714/arguments.txt\\ndeleted file mode 100644\\nindex b9316b0..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-115714/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115714/events.out.tfevents.1586845656.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-115714/events.out.tfevents.1586845656.rao-B360M-D2V\\ndeleted file mode 100644\\nindex b08cd84..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-115714/events.out.tfevents.1586845656.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115714/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-115714/revision_info.txt\\ndeleted file mode 100644\\nindex b7af341..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-115714/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..f481fc7 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=3)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=10)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-120909/arguments.txt b/faceNet-yolo/logs/facenet/20200414-120909/arguments.txt\\ndeleted file mode 100644\\nindex b9316b0..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-120909/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-120909/events.out.tfevents.1586846371.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-120909/events.out.tfevents.1586846371.rao-B360M-D2V\\ndeleted file mode 100644\\nindex a6bcc4a..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-120909/events.out.tfevents.1586846371.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-120909/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-120909/revision_info.txt\\ndeleted file mode 100644\\nindex b7af341..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-120909/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..f481fc7 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=3)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=10)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-121751/arguments.txt b/faceNet-yolo/logs/facenet/20200414-121751/arguments.txt\\ndeleted file mode 100644\\nindex b9316b0..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-121751/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-121751/events.out.tfevents.1586846892.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-121751/events.out.tfevents.1586846892.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 910c3de..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-121751/events.out.tfevents.1586846892.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-121751/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-121751/revision_info.txt\\ndeleted file mode 100644\\nindex 5081e94..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-121751/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..bd2b3b9 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -135,6 +135,7 @@ def main(args):\\\\n         \\\\n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\'embeddings\\\\\\\')\\\\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\\\\n+        print("EMBEDING S ===============================> ",embeddings)\\\\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\\\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\\\n         \\\\n@@ -219,9 +220,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +445,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=3)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=10)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171006/arguments.txt b/faceNet-yolo/logs/facenet/20200414-171006/arguments.txt\\ndeleted file mode 100644\\nindex a748622..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-171006/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 1\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171006/events.out.tfevents.1586864428.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-171006/events.out.tfevents.1586864428.rao-B360M-D2V\\ndeleted file mode 100644\\nindex f38210b..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-171006/events.out.tfevents.1586864428.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171006/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-171006/revision_info.txt\\ndeleted file mode 100644\\nindex e3d6aa1..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-171006/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..e5a8f76 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -135,6 +135,7 @@ def main(args):\\\\n         \\\\n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\'embeddings\\\\\\\')\\\\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\\\\n+        print("EMBEDING S ===============================> ",embeddings)\\\\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\\\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\\\n         \\\\n@@ -219,9 +220,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +445,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=10)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171104/arguments.txt b/faceNet-yolo/logs/facenet/20200414-171104/arguments.txt\\ndeleted file mode 100644\\nindex b9316b0..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-171104/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171104/events.out.tfevents.1586864486.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-171104/events.out.tfevents.1586864486.rao-B360M-D2V\\ndeleted file mode 100644\\nindex 0eb662a..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-171104/events.out.tfevents.1586864486.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171104/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-171104/revision_info.txt\\ndeleted file mode 100644\\nindex 5081e94..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-171104/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..bd2b3b9 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -135,6 +135,7 @@ def main(args):\\\\n         \\\\n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\'embeddings\\\\\\\')\\\\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\\\\n+        print("EMBEDING S ===============================> ",embeddings)\\\\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\\\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\\\n         \\\\n@@ -219,9 +220,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -440,13 +445,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=3)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=10)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-174305/arguments.txt b/faceNet-yolo/logs/facenet/20200414-174305/arguments.txt\\ndeleted file mode 100644\\nindex b9316b0..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-174305/arguments.txt\\n+++ /dev/null\\n@@ -1,28 +0,0 @@\\n-logs_base_dir: ./logs/facenet\\n-models_base_dir: ./models/facenet\\n-gpu_memory_fraction: 1\\n-pretrained_model: None\\n-data_dir: ./aligned_faces\\n-model_def: models.inception_resnet_v1\\n-max_nrof_epochs: 50\\n-batch_size: 11\\n-image_size: 160\\n-people_per_batch: 3\\n-images_per_person: 10\\n-epoch_size: 250\\n-alpha: 0.3\\n-embedding_size: 128\\n-random_crop: False\\n-random_flip: False\\n-keep_probability: 0.8\\n-weight_decay: 0.0\\n-optimizer: ADAGRAD\\n-learning_rate: 0.1\\n-learning_rate_decay_epochs: 100\\n-learning_rate_decay_factor: 1.0\\n-moving_average_decay: 0.9999\\n-seed: 666\\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\\n-lfw_pairs: data/pairs.txt\\n-lfw_dir: \\n-lfw_nrof_folds: 10\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-174305/events.out.tfevents.1586866407.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-174305/events.out.tfevents.1586866407.rao-B360M-D2V\\ndeleted file mode 100644\\nindex b9ee004..0000000\\nBinary files a/faceNet-yolo/logs/facenet/20200414-174305/events.out.tfevents.1586866407.rao-B360M-D2V and /dev/null differ\\ndiff --git a/faceNet-yolo/logs/facenet/20200414-174305/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-174305/revision_info.txt\\ndeleted file mode 100644\\nindex e966e8e..0000000\\n--- a/faceNet-yolo/logs/facenet/20200414-174305/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: train_tripleloss.py\\n---------------------\\n-tensorflow version: 1.13.2\\n---------------------\\n-git hash: b\\\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\\\'\\n---------------------\\n-b\\\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\\\nindex 8aa27cb..79f793e 100644\\\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\\\nindex e6eaf5a..e384625 100644\\\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\\\nindex 148e150..fe75216 100644\\\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\\\nindex e163d74..8e2fd93 100644\\\\n--- a/faceNet-yolo/train_tripleloss.py\\\\n+++ b/faceNet-yolo/train_tripleloss.py\\\\n@@ -135,6 +135,7 @@ def main(args):\\\\n         \\\\n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\'embeddings\\\\\\\')\\\\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\\\\n+        print("EMBEDING S ===============================> ",embeddings)\\\\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\\\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\\\n         \\\\n@@ -219,9 +220,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\\\n         print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n         start_time = time.time()\\\\n         nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        print("nrof_examples =========+>", nrof_examples)\\\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n+        # labels_array = np.arange(nrof_examples)\\\\n+        print("DONEEEE labels_array=========+>", labels_array)\\\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n@@ -361,6 +366,7 @@ def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholde\\\\n     label_check_array = np.zeros((nrof_images,))\\\\n     for i in xrange(nrof_batches):\\\\n         batch_size = min(nrof_images-i*batch_size, batch_size)\\\\n+        print("Batch size ===========> ", batch_size)\\\\n         emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\\\\n             learning_rate_placeholder: 0.0, phase_train_placeholder: False})\\\\n         emb_array[lab,:] = emb\\\\n@@ -440,13 +446,13 @@ def parse_arguments(argv):\\\\n     parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of epochs to run.\\\\\\\', default=50)\\\\n     parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=11)\\\\n     parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n     parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of people per batch.\\\\\\\', default=3)\\\\n     parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=1)\\\\n+        help=\\\\\\\'Number of images per person.\\\\\\\', default=10)\\\\n     parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n         help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=250)\\\\n     parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\'\\n\\\\ No newline at end of file\\ndiff --git a/faceNet-yolo/make-classifier.py b/faceNet-yolo/make-classifier.py\\nindex 53e59b7..976210b 100644\\n--- a/faceNet-yolo/make-classifier.py\\n+++ b/faceNet-yolo/make-classifier.py\\n@@ -22,7 +22,8 @@ with tf.Graph().as_default():\\n         print(\\\'Number of images: %d\\\' % len(paths))\\n \\n         print(\\\'Loading feature extraction model\\\')\\n-        modeldir = \\\'./models/facenet/20180402-114759\\\'\\n+        modeldir = \\\'./models/20170512-110547-working-model\\\'\\n+        print("model dir ===========> ", modeldir)\\n         facenet.load_model(modeldir)\\n \\n         images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\ndiff --git a/faceNet-yolo/models/facenet/20180402-114759/20180402-114759.pb b/faceNet-yolo/models/facenet/20180402-114759/20180402-114759.pb\\ndeleted file mode 100644\\nindex 39b4ed7..0000000\\nBinary files a/faceNet-yolo/models/facenet/20180402-114759/20180402-114759.pb and /dev/null differ\\ndiff --git a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.data-00000-of-00001 b/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.data-00000-of-00001\\ndeleted file mode 100755\\nindex 6160198..0000000\\nBinary files a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.data-00000-of-00001 and /dev/null differ\\ndiff --git a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.index b/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.index\\ndeleted file mode 100755\\nindex e2b346c..0000000\\nBinary files a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.index and /dev/null differ\\ndiff --git a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.meta b/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.meta\\ndeleted file mode 100755\\nindex abffaef..0000000\\nBinary files a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.meta and /dev/null differ\\ndiff --git a/faceNet-yolo/myclassifier/my_classifier.pkl b/faceNet-yolo/myclassifier/my_classifier.pkl\\nindex f531d3c..407e8a2 100644\\nBinary files a/faceNet-yolo/myclassifier/my_classifier.pkl and b/faceNet-yolo/myclassifier/my_classifier.pkl differ\\ndiff --git a/faceNet-yolo/realtime_facenet.py b/faceNet-yolo/realtime_facenet.py\\nindex 6d0815b..37a8140 100644\\n--- a/faceNet-yolo/realtime_facenet.py\\n+++ b/faceNet-yolo/realtime_facenet.py\\n@@ -13,6 +13,7 @@ import detect_face\\n import os\\n import time\\n import pickle\\n+global str\\n \\n \\n \\n@@ -35,7 +36,8 @@ with tf.Graph().as_default():\\n         # HumanNames = [\\\'Andrew\\\',\\\'Obama\\\',\\\'ZiLin\\\']    #train human name\\n \\n         print(\\\'Loading feature extraction model\\\')\\n-        modeldir = \\\'./models/facenet/20180402-114759\\\'\\n+        modeldir = \\\'./models/20170512-110547-new\\\'  #pre-trained model\\n+        # modeldir = \\\'./models/facenet/20200415-171023\\\'    #Self-trained model   \\n         facenet.load_model(modeldir)\\n \\n         images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n@@ -57,107 +59,110 @@ with tf.Graph().as_default():\\n         #     cv2.imshow(\\\'IPWebcam\\\',img)\\n         #     if cv2.waitKey(1) == 27:\\n         #         break\\n-        url=\\\'http://192.168.43.1:8080/shot.jpg\\\'\\n+        url=\\\'http://192.168.43.1:8080/video\\\'\\n         video_capture = cv2.VideoCapture(url)\\n         c = 0\\n \\n         print(\\\'Start Recognition!\\\')\\n         prevTime = 0\\n-        \\n-        ret, frame = video_capture.read()\\n-        print("ret, frame ==================>", ret, frame)\\n-        # frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)    #resize frame (optional)\\n-\\n-        curTime = time.time()    # calc fps\\n-        timeF = frame_interval\\n-\\n-        if (c % timeF == 0):\\n-            find_results = []\\n-\\n-            if frame.ndim == 2:\\n-                frame = facenet.to_rgb(frame)\\n-            frame = frame[:, :, 0:3]\\n-            #print(frame.shape[0])\\n-            #print(frame.shape[1])\\n-\\n-            ## Use MTCNN to get the bounding boxes\\n-            bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor)\\n-            nrof_faces = bounding_boxes.shape[0]\\n-            #print(\\\'Detected_FaceNum: %d\\\' % nrof_faces)\\n-\\n-            if nrof_faces > 0:\\n-                det = bounding_boxes[:, 0:4]\\n-                img_size = np.asarray(frame.shape)[0:2]\\n-\\n-                # cropped = []\\n-                # scaled = []\\n-                # scaled_reshape = []\\n-                bb = np.zeros((nrof_faces,4), dtype=np.int32)\\n-\\n-                for i in range(nrof_faces):\\n-                    emb_array = np.zeros((1, embedding_size))\\n-\\n-                    bb[i][0] = det[i][0]\\n-                    bb[i][1] = det[i][1]\\n-                    bb[i][2] = det[i][2]\\n-                    bb[i][3] = det[i][3]\\n-\\n-                    if bb[i][0] <= 0 or bb[i][1] <= 0 or bb[i][2] >= len(frame[0]) or bb[i][3] >= len(frame):\\n-                        print(\\\'face is inner of range!\\\')\\n-                        continue\\n-\\n-                    # cropped.append(frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\\n-                    # cropped[0] = facenet.flip(cropped[0], False)\\n-                    # scaled.append(misc.imresize(cropped[0], (image_size, image_size), interp=\\\'bilinear\\\'))\\n-                    # scaled[0] = cv2.resize(scaled[0], (input_image_size,input_image_size),\\n-                    #                        interpolation=cv2.INTER_CUBIC)\\n-                    # scaled[0] = facenet.prewhiten(scaled[0])\\n-                    # scaled_reshape.append(scaled[0].reshape(-1,input_image_size,input_image_size,3))\\n-                    # feed_dict = {images_placeholder: scaled_reshape[0], phase_train_placeholder: False}\\n-\\n-                    cropped = (frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\\n-                    print("{0} {1} {2} {3}".format(bb[i][0], bb[i][1], bb[i][2], bb[i][3]))\\n-                    cropped = facenet.flip(cropped, False)\\n-                    scaled = (misc.imresize(cropped, (image_size, image_size), interp=\\\'bilinear\\\'))\\n-                    scaled = cv2.resize(scaled, (input_image_size,input_image_size),\\n-                                        interpolation=cv2.INTER_CUBIC)\\n-                    scaled = facenet.prewhiten(scaled)\\n-                    scaled_reshape = (scaled.reshape(-1,input_image_size,input_image_size,3))\\n-                    feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\n-\\n-                    emb_array[0, :] = sess.run(embeddings, feed_dict=feed_dict)\\n-\\n-                    predictions = model.predict_proba(emb_array)\\n-                    best_class_indices = np.argmax(predictions, axis=1)\\n-                    best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\n-\\n-                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\n-                    text_x = bb[i][0]\\n-                    text_y = bb[i][3] + 20\\n-                    # for H_i in HumanNames:\\n-                    #     if HumanNames[best_class_indices[0]] == H_i:\\n-                    result_names = class_names[best_class_indices[0]]\\n-                    print("best_class_probabilities ==> ", best_class_probabilities, "best_class_indices ===>", best_class_indices, "result name ===>", result_names , "predictions =====> ", predictions)\\n-\\n-                    #print(result_names)\\n-                    cv2.putText(frame, result_names, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                1, (0, 0, 255), thickness=1, lineType=2)\\n-            else:\\n-                print(\\\'Unable to align\\\')\\n-\\n-        sec = curTime - prevTime\\n-        prevTime = curTime\\n-        fps = 1 / (sec)\\n-        str = \\\'FPS: %2.3f\\\' % fps\\n-        text_fps_x = len(frame[0]) - 150\\n-        text_fps_y = 20\\n-        cv2.putText(frame, str, (text_fps_x, text_fps_y),\\n-                    cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 0), thickness=1, lineType=2)\\n-        # c+=1\\n-        # print("facme =========> ", frame)\\n-\\n-        cv2.imshow(\\\'Video\\\', frame)\\n-\\n+        while True:\\n+            ret, frame = video_capture.read()\\n+            # print("frame =================> ", frame)\\n+            # print("frame.ndim =================> ", frame.ndim)\\n+            frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)    #resize frame (optional)\\n+\\n+            curTime = time.time()    # calc fps\\n+            timeF = frame_interval\\n+\\n+            if (c % timeF == 0):\\n+                find_results = []\\n+\\n+                if frame.ndim == 2:\\n+                    frame = facenet.to_rgb(frame)\\n+                frame = frame[:, :, 0:3]\\n+                #print(frame.shape[0])\\n+                #print(frame.shape[1])\\n+\\n+                ## Use MTCNN to get the bounding boxes\\n+                bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor)\\n+                nrof_faces = bounding_boxes.shape[0]\\n+                #print(\\\'Detected_FaceNum: %d\\\' % nrof_faces)\\n+\\n+                if nrof_faces > 0:\\n+                    det = bounding_boxes[:, 0:4]\\n+                    img_size = np.asarray(frame.shape)[0:2]\\n+\\n+                    # cropped = []\\n+                    # scaled = []\\n+                    # scaled_reshape = []\\n+                    bb = np.zeros((nrof_faces,4), dtype=np.int32)\\n+\\n+                    for i in range(nrof_faces):\\n+                        emb_array = np.zeros((1, embedding_size))\\n+\\n+                        bb[i][0] = det[i][0]\\n+                        bb[i][1] = det[i][1]\\n+                        bb[i][2] = det[i][2]\\n+                        bb[i][3] = det[i][3]\\n+\\n+                        if bb[i][0] <= 0 or bb[i][1] <= 0 or bb[i][2] >= len(frame[0]) or bb[i][3] >= len(frame):\\n+                            print(\\\'face is inner of range!\\\')\\n+                            continue\\n+\\n+                        # cropped.append(frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\\n+                        # cropped[0] = facenet.flip(cropped[0], False)\\n+                        # scaled.append(misc.imresize(cropped[0], (image_size, image_size), interp=\\\'bilinear\\\'))\\n+                        # scaled[0] = cv2.resize(scaled[0], (input_image_size,input_image_size),\\n+                        #                        interpolation=cv2.INTER_CUBIC)\\n+                        # scaled[0] = facenet.prewhiten(scaled[0])\\n+                        # scaled_reshape.append(scaled[0].reshape(-1,input_image_size,input_image_size,3))\\n+                        # feed_dict = {images_placeholder: scaled_reshape[0], phase_train_placeholder: False}\\n+\\n+                        cropped = (frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\\n+                        print("{0} {1} {2} {3}".format(bb[i][0], bb[i][1], bb[i][2], bb[i][3]))\\n+                        cropped = facenet.flip(cropped, False)\\n+                        scaled = (misc.imresize(cropped, (image_size, image_size), interp=\\\'bilinear\\\'))\\n+                        scaled = cv2.resize(scaled, (input_image_size,input_image_size),\\n+                                            interpolation=cv2.INTER_CUBIC)\\n+                        scaled = facenet.prewhiten(scaled)\\n+                        scaled_reshape = (scaled.reshape(-1,input_image_size,input_image_size,3))\\n+                        feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\n+\\n+                        emb_array[0, :] = sess.run(embeddings, feed_dict=feed_dict)\\n+\\n+                        predictions = model.predict_proba(emb_array)\\n+                        best_class_indices = np.argmax(predictions, axis=1)\\n+                        best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\n+                        cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\n+                        text_x = bb[i][0]\\n+                        text_y = bb[i][3] + 20\\n+\\n+                        # for H_i in HumanNames:\\n+                        #     if HumanNames[best_class_indices[0]] == H_i:\\n+                        result_names = class_names[best_class_indices[0]]\\n+                        # strprob =  str(best_class_probabilities[0])\\n+                        # result_names = result_names  + strprob\\n+                        print("best_class_probabilities ==> ", best_class_probabilities[0], "best_class_indices ===>", best_class_indices, "result name ===>", result_names , "predictions =====> ", predictions)\\n+                        #print(result_names)\\n+                        cv2.putText(frame, result_names, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n+                                    1, (0, 0, 255), thickness=1, lineType=2)\\n+                else:\\n+                    print(\\\'Unable to align\\\')\\n+\\n+            sec = curTime - prevTime\\n+            prevTime = curTime\\n+            fps = 1 / (sec)\\n+            str = \\\'FPS: %2.3f\\\' % fps\\n+            text_fps_x = len(frame[0]) - 150\\n+            text_fps_y = 20\\n+            cv2.putText(frame, str, (text_fps_x, text_fps_y),\\n+                        cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 0), thickness=1, lineType=2)\\n+            # c+=1\\n+\\n+            cv2.imshow(\\\'Video\\\', frame)\\n+\\n+            if cv2.waitKey(1) & 0xFF == ord(\\\'q\\\'):\\n+                break\\n \\n         video_capture.release()\\n         # #video writer\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex 8e2fd93..898de6f 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -444,9 +444,9 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--model_def\\\', type=str,\\n         help=\\\'Model definition. Points to a module containing the definition of the inference graph.\\\', default=\\\'models.inception_resnet_v1\\\')\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n-        help=\\\'Number of epochs to run.\\\', default=50)\\n+        help=\\\'Number of epochs to run.\\\', default=3)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=11)\\n+        help=\\\'Number of images to process in a batch.\\\', default=15)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n@@ -454,7 +454,7 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n         help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n-        help=\\\'Number of batches per epoch.\\\', default=250)\\n+        help=\\\'Number of batches per epoch.\\\', default=10)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\\n         help=\\\'Positive to negative triplet distance margin.\\\', default=0.3)\\n     parser.add_argument(\\\'--embedding_size\\\', type=int,\\ndiff --git a/faceNet-yolo/unaligned_faces/s1/pr_train_1.jpg b/faceNet-yolo/unaligned_faces/s1/pr_train_1.jpg\\ndeleted file mode 100644\\nindex ed8e38a..0000000\\nBinary files a/faceNet-yolo/unaligned_faces/s1/pr_train_1.jpg and /dev/null differ\\ndiff --git a/faceNet-yolo/unaligned_faces/s2/ishita1.jpg b/faceNet-yolo/unaligned_faces/s2/ishita1.jpg\\ndeleted file mode 100644\\nindex 014fcb5..0000000\\nBinary files a/faceNet-yolo/unaligned_faces/s2/ishita1.jpg and /dev/null differ\\ndiff --git a/faceNet-yolo/unaligned_faces/s3/papa2.jpg b/faceNet-yolo/unaligned_faces/s3/papa2.jpg\\ndeleted file mode 100644\\nindex 7c3ff3a..0000000\\nBinary files a/faceNet-yolo/unaligned_faces/s3/papa2.jpg and /dev/null differ\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/aligned_faces/s1/pr_train_1.png b/faceNet-yolo/aligned_faces/s1/pr_train_1.png\ndeleted file mode 100644\nindex ea58b77..0000000\nBinary files a/faceNet-yolo/aligned_faces/s1/pr_train_1.png and /dev/null differ\ndiff --git a/faceNet-yolo/aligned_faces/s2/ishita1.png b/faceNet-yolo/aligned_faces/s2/ishita1.png\ndeleted file mode 100644\nindex fc33eb6..0000000\nBinary files a/faceNet-yolo/aligned_faces/s2/ishita1.png and /dev/null differ\ndiff --git a/faceNet-yolo/aligned_faces/s3/papa2.png b/faceNet-yolo/aligned_faces/s3/papa2.png\ndeleted file mode 100644\nindex d1c2694..0000000\nBinary files a/faceNet-yolo/aligned_faces/s3/papa2.png and /dev/null differ\ndiff --git a/faceNet-yolo/facenet.py b/faceNet-yolo/facenet.py\nindex fc7da25..c2b2b44 100644\n--- a/faceNet-yolo/facenet.py\n+++ b/faceNet-yolo/facenet.py\n@@ -363,6 +363,7 @@ def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n     return train_set, test_set\n \n def load_model(model, input_map=None):\n+    print(" ==============> ", model)\n     # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n     #  or if it is a protobuf file with a frozen graph\n     model_exp = os.path.expanduser(model)\ndiff --git a/faceNet-yolo/logs/facenet/20200413-164246/arguments.txt b/faceNet-yolo/logs/facenet/20200413-164246/arguments.txt\ndeleted file mode 100644\nindex 4394f23..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-164246/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 15\n-image_size: 160\n-people_per_batch: 6\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-164246/events.out.tfevents.1586776388.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-164246/events.out.tfevents.1586776388.rao-B360M-D2V\ndeleted file mode 100644\nindex 73b6ba9..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-164246/events.out.tfevents.1586776388.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-164246/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-164246/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-164246/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-165512/arguments.txt b/faceNet-yolo/logs/facenet/20200413-165512/arguments.txt\ndeleted file mode 100644\nindex 139f2e3..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-165512/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-165512/events.out.tfevents.1586777134.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-165512/events.out.tfevents.1586777134.rao-B360M-D2V\ndeleted file mode 100644\nindex 114ee3b..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-165512/events.out.tfevents.1586777134.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-165512/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-165512/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-165512/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170019/arguments.txt b/faceNet-yolo/logs/facenet/20200413-170019/arguments.txt\ndeleted file mode 100644\nindex 6f47b3d..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-170019/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 1\n-epoch_size: 1\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170019/events.out.tfevents.1586777441.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-170019/events.out.tfevents.1586777441.rao-B360M-D2V\ndeleted file mode 100644\nindex f8ac3b5..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-170019/events.out.tfevents.1586777441.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170019/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-170019/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-170019/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170139/arguments.txt b/faceNet-yolo/logs/facenet/20200413-170139/arguments.txt\ndeleted file mode 100644\nindex 6f47b3d..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-170139/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 1\n-epoch_size: 1\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170139/events.out.tfevents.1586777534.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-170139/events.out.tfevents.1586777534.rao-B360M-D2V\ndeleted file mode 100644\nindex eaa431f..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-170139/events.out.tfevents.1586777534.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170139/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-170139/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-170139/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170533/arguments.txt b/faceNet-yolo/logs/facenet/20200413-170533/arguments.txt\ndeleted file mode 100644\nindex 6f47b3d..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-170533/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 1\n-epoch_size: 1\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170533/events.out.tfevents.1586777788.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-170533/events.out.tfevents.1586777788.rao-B360M-D2V\ndeleted file mode 100644\nindex 2ab7a3b..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-170533/events.out.tfevents.1586777788.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170533/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-170533/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-170533/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170838/arguments.txt b/faceNet-yolo/logs/facenet/20200413-170838/arguments.txt\ndeleted file mode 100644\nindex 6f47b3d..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-170838/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 1\n-epoch_size: 1\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170838/events.out.tfevents.1586777970.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-170838/events.out.tfevents.1586777970.rao-B360M-D2V\ndeleted file mode 100644\nindex 55e0a58..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-170838/events.out.tfevents.1586777970.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-170838/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-170838/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-170838/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-171326/arguments.txt b/faceNet-yolo/logs/facenet/20200413-171326/arguments.txt\ndeleted file mode 100644\nindex bf8332a..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-171326/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./output\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 15\n-image_size: 160\n-people_per_batch: 6\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-171326/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-171326/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-171326/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-180801/arguments.txt b/faceNet-yolo/logs/facenet/20200413-180801/arguments.txt\ndeleted file mode 100644\nindex 6fb7f7a..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-180801/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 15\n-image_size: 160\n-people_per_batch: 6\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-180801/events.out.tfevents.1586781539.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-180801/events.out.tfevents.1586781539.rao-B360M-D2V\ndeleted file mode 100644\nindex 4e7dc8a..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-180801/events.out.tfevents.1586781539.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-180801/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-180801/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-180801/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181225/arguments.txt b/faceNet-yolo/logs/facenet/20200413-181225/arguments.txt\ndeleted file mode 100644\nindex cf47e42..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-181225/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 15\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181225/events.out.tfevents.1586781768.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-181225/events.out.tfevents.1586781768.rao-B360M-D2V\ndeleted file mode 100644\nindex 961837f..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-181225/events.out.tfevents.1586781768.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181225/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-181225/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-181225/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181822/arguments.txt b/faceNet-yolo/logs/facenet/20200413-181822/arguments.txt\ndeleted file mode 100644\nindex cf47e42..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-181822/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 15\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181822/events.out.tfevents.1586782124.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-181822/events.out.tfevents.1586782124.rao-B360M-D2V\ndeleted file mode 100644\nindex cdba8c7..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-181822/events.out.tfevents.1586782124.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-181822/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-181822/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-181822/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184739/arguments.txt b/faceNet-yolo/logs/facenet/20200413-184739/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-184739/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184739/events.out.tfevents.1586783885.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-184739/events.out.tfevents.1586783885.rao-B360M-D2V\ndeleted file mode 100644\nindex 3341c00..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-184739/events.out.tfevents.1586783885.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184739/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-184739/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-184739/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184938/arguments.txt b/faceNet-yolo/logs/facenet/20200413-184938/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-184938/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184938/events.out.tfevents.1586784005.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-184938/events.out.tfevents.1586784005.rao-B360M-D2V\ndeleted file mode 100644\nindex 88d6220..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-184938/events.out.tfevents.1586784005.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-184938/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-184938/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-184938/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200413-185114/arguments.txt b/faceNet-yolo/logs/facenet/20200413-185114/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-185114/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200413-185114/events.out.tfevents.1586784102.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200413-185114/events.out.tfevents.1586784102.rao-B360M-D2V\ndeleted file mode 100644\nindex 2f7096a..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200413-185114/events.out.tfevents.1586784102.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200413-185114/revision_info.txt b/faceNet-yolo/logs/facenet/20200413-185114/revision_info.txt\ndeleted file mode 100644\nindex f8acf7b..0000000\n--- a/faceNet-yolo/logs/facenet/20200413-185114/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'7c7dadf82d042710349852385539f58303124612\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-085926/arguments.txt b/faceNet-yolo/logs/facenet/20200414-085926/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-085926/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-085926/events.out.tfevents.1586834990.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-085926/events.out.tfevents.1586834990.rao-B360M-D2V\ndeleted file mode 100644\nindex 88c44bd..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-085926/events.out.tfevents.1586834990.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-085926/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-085926/revision_info.txt\ndeleted file mode 100644\nindex 7efc726..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-085926/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090615/arguments.txt b/faceNet-yolo/logs/facenet/20200414-090615/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-090615/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090615/events.out.tfevents.1586835399.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-090615/events.out.tfevents.1586835399.rao-B360M-D2V\ndeleted file mode 100644\nindex e1e64cf..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-090615/events.out.tfevents.1586835399.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090615/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-090615/revision_info.txt\ndeleted file mode 100644\nindex b4d80e3..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-090615/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..2d28df3 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -243,6 +243,7 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         # Perform training on the selected triplets\\n         nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\\n         triplet_paths = list(itertools.chain(*triplets))\\n+        print("triplet_paths =====> ", triplet_paths, len(triplet_paths))\\n         labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\\n         triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\\n         sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090756/arguments.txt b/faceNet-yolo/logs/facenet/20200414-090756/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-090756/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090756/events.out.tfevents.1586835499.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-090756/events.out.tfevents.1586835499.rao-B360M-D2V\ndeleted file mode 100644\nindex 6ccfb65..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-090756/events.out.tfevents.1586835499.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090756/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-090756/revision_info.txt\ndeleted file mode 100644\nindex 94c4d2d..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-090756/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..97c8a9d 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -220,6 +220,8 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n         print("nrof_examples =========+>", nrof_examples)\\n+        print("triplet_paths =====> ", nrof_examples, len(nrof_examples))\\n+        \\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090854/arguments.txt b/faceNet-yolo/logs/facenet/20200414-090854/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-090854/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090854/events.out.tfevents.1586835556.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-090854/events.out.tfevents.1586835556.rao-B360M-D2V\ndeleted file mode 100644\nindex d567305..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-090854/events.out.tfevents.1586835556.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-090854/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-090854/revision_info.txt\ndeleted file mode 100644\nindex 728dd00..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-090854/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..9547975 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -220,6 +220,8 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n         print("nrof_examples =========+>", nrof_examples)\\n+        print("triplet_paths =====> ", nrof_examples)\\n+\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091110/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091110/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091110/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091110/events.out.tfevents.1586835691.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091110/events.out.tfevents.1586835691.rao-B360M-D2V\ndeleted file mode 100644\nindex 36ca54e..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-091110/events.out.tfevents.1586835691.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091110/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091110/revision_info.txt\ndeleted file mode 100644\nindex 2953a43..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091110/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..1470414 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -220,7 +220,8 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n         print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\n+        print("DONEEEE=========+>")\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091208/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091208/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091208/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091208/events.out.tfevents.1586835750.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091208/events.out.tfevents.1586835750.rao-B360M-D2V\ndeleted file mode 100644\nindex 37b5194..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-091208/events.out.tfevents.1586835750.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091208/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091208/revision_info.txt\ndeleted file mode 100644\nindex a35b0d7..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091208/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..f74df72 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,8 +219,9 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\n+        print("DONEEEE=========+>")\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091502/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091502/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091502/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091502/events.out.tfevents.1586835924.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091502/events.out.tfevents.1586835924.rao-B360M-D2V\ndeleted file mode 100644\nindex 80f853f..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-091502/events.out.tfevents.1586835924.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091502/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091502/revision_info.txt\ndeleted file mode 100644\nindex 834b914..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091502/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..ab0c4a3 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,8 +219,10 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\n+        labels_array = np.arange(nrof_examples).reshape(-1,3)\\n+        print("DONEEEE=========+>")\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091650/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091650/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091650/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091650/events.out.tfevents.1586836034.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091650/events.out.tfevents.1586836034.rao-B360M-D2V\ndeleted file mode 100644\nindex 1110554..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-091650/events.out.tfevents.1586836034.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091650/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091650/revision_info.txt\ndeleted file mode 100644\nindex f04b848..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091650/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..ec0fa70 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,8 +219,10 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\n+        labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE=========+>")\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091826/arguments.txt b/faceNet-yolo/logs/facenet/20200414-091826/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091826/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091826/events.out.tfevents.1586836130.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-091826/events.out.tfevents.1586836130.rao-B360M-D2V\ndeleted file mode 100644\nindex a0fa18e..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-091826/events.out.tfevents.1586836130.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-091826/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-091826/revision_info.txt\ndeleted file mode 100644\nindex d8e2271..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-091826/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..27cb427 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\n+        labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n+        # image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array)\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092010/arguments.txt b/faceNet-yolo/logs/facenet/20200414-092010/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-092010/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092010/events.out.tfevents.1586836234.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-092010/events.out.tfevents.1586836234.rao-B360M-D2V\ndeleted file mode 100644\nindex 207897d..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-092010/events.out.tfevents.1586836234.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092010/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-092010/revision_info.txt\ndeleted file mode 100644\nindex e3ed4f3..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-092010/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..a660837 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,10 +219,14 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\n+        labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n+        # image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n+        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array[0], labels_placeholder: labels_array[0]})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n         for i in range(nrof_batches):\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092815/arguments.txt b/faceNet-yolo/logs/facenet/20200414-092815/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-092815/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092815/events.out.tfevents.1586836719.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-092815/events.out.tfevents.1586836719.rao-B360M-D2V\ndeleted file mode 100644\nindex dbca194..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-092815/events.out.tfevents.1586836719.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-092815/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-092815/revision_info.txt\ndeleted file mode 100644\nindex 3d41b8a..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-092815/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..d60aca4 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        # labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\n+        labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n+        # image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-093742/arguments.txt b/faceNet-yolo/logs/facenet/20200414-093742/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-093742/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-093742/events.out.tfevents.1586837286.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-093742/events.out.tfevents.1586837286.rao-B360M-D2V\ndeleted file mode 100644\nindex 3b6249d..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-093742/events.out.tfevents.1586837286.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-093742/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-093742/revision_info.txt\ndeleted file mode 100644\nindex 1ed16ef..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-093742/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..d795492 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,2))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-094136/arguments.txt b/faceNet-yolo/logs/facenet/20200414-094136/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-094136/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-094136/events.out.tfevents.1586837520.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-094136/events.out.tfevents.1586837520.rao-B360M-D2V\ndeleted file mode 100644\nindex e413aef..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-094136/events.out.tfevents.1586837520.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-094136/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-094136/revision_info.txt\ndeleted file mode 100644\nindex 61bb5ac..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-094136/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..4f9c298 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,0))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095012/arguments.txt b/faceNet-yolo/logs/facenet/20200414-095012/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-095012/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095012/events.out.tfevents.1586838034.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-095012/events.out.tfevents.1586838034.rao-B360M-D2V\ndeleted file mode 100644\nindex e6a9fa9..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-095012/events.out.tfevents.1586838034.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095012/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-095012/revision_info.txt\ndeleted file mode 100644\nindex 3a805c3..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-095012/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..e67be7e 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n+        labels_array = np.reshape(np.arange(nrof_examples),(-1,1))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n+        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,1))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095136/arguments.txt b/faceNet-yolo/logs/facenet/20200414-095136/arguments.txt\ndeleted file mode 100644\nindex 0da86de..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-095136/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 1\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095136/events.out.tfevents.1586838119.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-095136/events.out.tfevents.1586838119.rao-B360M-D2V\ndeleted file mode 100644\nindex 6c98d53..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-095136/events.out.tfevents.1586838119.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-095136/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-095136/revision_info.txt\ndeleted file mode 100644\nindex f2f0b54..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-095136/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..7fbd596 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-113857/arguments.txt b/faceNet-yolo/logs/facenet/20200414-113857/arguments.txt\ndeleted file mode 100644\nindex 4394f23..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-113857/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 15\n-image_size: 160\n-people_per_batch: 6\n-images_per_person: 1\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-113857/events.out.tfevents.1586844559.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-113857/events.out.tfevents.1586844559.rao-B360M-D2V\ndeleted file mode 100644\nindex 982aa82..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-113857/events.out.tfevents.1586844559.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-113857/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-113857/revision_info.txt\ndeleted file mode 100644\nindex 9cacfaf..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-113857/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..f677692 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,11 +444,11 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=15)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=6)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n         help=\\\'Number of images per person.\\\', default=1)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114001/arguments.txt b/faceNet-yolo/logs/facenet/20200414-114001/arguments.txt\ndeleted file mode 100644\nindex 1834628..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-114001/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 15\n-image_size: 160\n-people_per_batch: 6\n-images_per_person: 11\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114001/events.out.tfevents.1586844623.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-114001/events.out.tfevents.1586844623.rao-B360M-D2V\ndeleted file mode 100644\nindex 6a42709..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-114001/events.out.tfevents.1586844623.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114001/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-114001/revision_info.txt\ndeleted file mode 100644\nindex b31acd8..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-114001/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..a51e814 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=15)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=6)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=11)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114146/arguments.txt b/faceNet-yolo/logs/facenet/20200414-114146/arguments.txt\ndeleted file mode 100644\nindex 05a0eb8..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-114146/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 15\n-image_size: 160\n-people_per_batch: 11\n-images_per_person: 11\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114146/events.out.tfevents.1586844728.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-114146/events.out.tfevents.1586844728.rao-B360M-D2V\ndeleted file mode 100644\nindex f173d44..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-114146/events.out.tfevents.1586844728.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114146/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-114146/revision_info.txt\ndeleted file mode 100644\nindex 995ccd5..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-114146/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..01410b9 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=15)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=11)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=11)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114742/arguments.txt b/faceNet-yolo/logs/facenet/20200414-114742/arguments.txt\ndeleted file mode 100644\nindex 3562145..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-114742/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 11\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114742/events.out.tfevents.1586845084.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-114742/events.out.tfevents.1586845084.rao-B360M-D2V\ndeleted file mode 100644\nindex dafb85c..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-114742/events.out.tfevents.1586845084.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-114742/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-114742/revision_info.txt\ndeleted file mode 100644\nindex 828779a..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-114742/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..fcf7912 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n         help=\\\'Number of people per batch.\\\', default=1)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=11)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115550/arguments.txt b/faceNet-yolo/logs/facenet/20200414-115550/arguments.txt\ndeleted file mode 100644\nindex 0a3ebff..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-115550/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 6\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115550/events.out.tfevents.1586845572.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-115550/events.out.tfevents.1586845572.rao-B360M-D2V\ndeleted file mode 100644\nindex a897ed7..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-115550/events.out.tfevents.1586845572.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115550/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-115550/revision_info.txt\ndeleted file mode 100644\nindex 9978d16..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-115550/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..8d26aea 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=6)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115629/arguments.txt b/faceNet-yolo/logs/facenet/20200414-115629/arguments.txt\ndeleted file mode 100644\nindex a748622..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-115629/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115629/events.out.tfevents.1586845611.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-115629/events.out.tfevents.1586845611.rao-B360M-D2V\ndeleted file mode 100644\nindex 129aa31..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-115629/events.out.tfevents.1586845611.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115629/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-115629/revision_info.txt\ndeleted file mode 100644\nindex e0c6c55..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-115629/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..82d19b8 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n         help=\\\'Number of people per batch.\\\', default=1)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115714/arguments.txt b/faceNet-yolo/logs/facenet/20200414-115714/arguments.txt\ndeleted file mode 100644\nindex b9316b0..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-115714/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115714/events.out.tfevents.1586845656.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-115714/events.out.tfevents.1586845656.rao-B360M-D2V\ndeleted file mode 100644\nindex b08cd84..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-115714/events.out.tfevents.1586845656.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-115714/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-115714/revision_info.txt\ndeleted file mode 100644\nindex b7af341..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-115714/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..f481fc7 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=3)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-120909/arguments.txt b/faceNet-yolo/logs/facenet/20200414-120909/arguments.txt\ndeleted file mode 100644\nindex b9316b0..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-120909/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-120909/events.out.tfevents.1586846371.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-120909/events.out.tfevents.1586846371.rao-B360M-D2V\ndeleted file mode 100644\nindex a6bcc4a..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-120909/events.out.tfevents.1586846371.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-120909/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-120909/revision_info.txt\ndeleted file mode 100644\nindex b7af341..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-120909/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..f481fc7 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -219,9 +219,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +444,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=3)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-121751/arguments.txt b/faceNet-yolo/logs/facenet/20200414-121751/arguments.txt\ndeleted file mode 100644\nindex b9316b0..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-121751/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-121751/events.out.tfevents.1586846892.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-121751/events.out.tfevents.1586846892.rao-B360M-D2V\ndeleted file mode 100644\nindex 910c3de..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-121751/events.out.tfevents.1586846892.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-121751/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-121751/revision_info.txt\ndeleted file mode 100644\nindex 5081e94..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-121751/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..bd2b3b9 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -135,6 +135,7 @@ def main(args):\\n         \\n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\'embeddings\\\')\\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\\n+        print("EMBEDING S ===============================> ",embeddings)\\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\n         \\n@@ -219,9 +220,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +445,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=3)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171006/arguments.txt b/faceNet-yolo/logs/facenet/20200414-171006/arguments.txt\ndeleted file mode 100644\nindex a748622..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-171006/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 1\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171006/events.out.tfevents.1586864428.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-171006/events.out.tfevents.1586864428.rao-B360M-D2V\ndeleted file mode 100644\nindex f38210b..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-171006/events.out.tfevents.1586864428.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171006/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-171006/revision_info.txt\ndeleted file mode 100644\nindex e3d6aa1..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-171006/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..e5a8f76 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -135,6 +135,7 @@ def main(args):\\n         \\n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\'embeddings\\\')\\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\\n+        print("EMBEDING S ===============================> ",embeddings)\\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\n         \\n@@ -219,9 +220,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +445,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n         help=\\\'Number of people per batch.\\\', default=1)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171104/arguments.txt b/faceNet-yolo/logs/facenet/20200414-171104/arguments.txt\ndeleted file mode 100644\nindex b9316b0..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-171104/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171104/events.out.tfevents.1586864486.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-171104/events.out.tfevents.1586864486.rao-B360M-D2V\ndeleted file mode 100644\nindex 0eb662a..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-171104/events.out.tfevents.1586864486.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-171104/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-171104/revision_info.txt\ndeleted file mode 100644\nindex 5081e94..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-171104/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..bd2b3b9 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -135,6 +135,7 @@ def main(args):\\n         \\n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\'embeddings\\\')\\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\\n+        print("EMBEDING S ===============================> ",embeddings)\\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\n         \\n@@ -219,9 +220,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -440,13 +445,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=3)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/logs/facenet/20200414-174305/arguments.txt b/faceNet-yolo/logs/facenet/20200414-174305/arguments.txt\ndeleted file mode 100644\nindex b9316b0..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-174305/arguments.txt\n+++ /dev/null\n@@ -1,28 +0,0 @@\n-logs_base_dir: ./logs/facenet\n-models_base_dir: ./models/facenet\n-gpu_memory_fraction: 1\n-pretrained_model: None\n-data_dir: ./aligned_faces\n-model_def: models.inception_resnet_v1\n-max_nrof_epochs: 50\n-batch_size: 11\n-image_size: 160\n-people_per_batch: 3\n-images_per_person: 10\n-epoch_size: 250\n-alpha: 0.3\n-embedding_size: 128\n-random_crop: False\n-random_flip: False\n-keep_probability: 0.8\n-weight_decay: 0.0\n-optimizer: ADAGRAD\n-learning_rate: 0.1\n-learning_rate_decay_epochs: 100\n-learning_rate_decay_factor: 1.0\n-moving_average_decay: 0.9999\n-seed: 666\n-learning_rate_schedule_file: data/learning_rate_schedule.txt\n-lfw_pairs: data/pairs.txt\n-lfw_dir: \n-lfw_nrof_folds: 10\ndiff --git a/faceNet-yolo/logs/facenet/20200414-174305/events.out.tfevents.1586866407.rao-B360M-D2V b/faceNet-yolo/logs/facenet/20200414-174305/events.out.tfevents.1586866407.rao-B360M-D2V\ndeleted file mode 100644\nindex b9ee004..0000000\nBinary files a/faceNet-yolo/logs/facenet/20200414-174305/events.out.tfevents.1586866407.rao-B360M-D2V and /dev/null differ\ndiff --git a/faceNet-yolo/logs/facenet/20200414-174305/revision_info.txt b/faceNet-yolo/logs/facenet/20200414-174305/revision_info.txt\ndeleted file mode 100644\nindex e966e8e..0000000\n--- a/faceNet-yolo/logs/facenet/20200414-174305/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: train_tripleloss.py\n---------------------\n-tensorflow version: 1.13.2\n---------------------\n-git hash: b\'20b8fe6c4cdb5628b3457a789c27b3e40277309d\'\n---------------------\n-b\'diff --git a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc\\nindex 8aa27cb..79f793e 100644\\nBinary files a/faceNet-yolo/__pycache__/facenet.cpython-36.pyc and b/faceNet-yolo/__pycache__/facenet.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc\\nindex e6eaf5a..e384625 100644\\nBinary files a/faceNet-yolo/__pycache__/lfw.cpython-36.pyc and b/faceNet-yolo/__pycache__/lfw.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc\\nindex 148e150..fe75216 100644\\nBinary files a/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc and b/faceNet-yolo/models/__pycache__/inception_resnet_v1.cpython-36.pyc differ\\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\\nindex e163d74..8e2fd93 100644\\n--- a/faceNet-yolo/train_tripleloss.py\\n+++ b/faceNet-yolo/train_tripleloss.py\\n@@ -135,6 +135,7 @@ def main(args):\\n         \\n         embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\'embeddings\\\')\\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\\n+        print("EMBEDING S ===============================> ",embeddings)\\n         anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\n         \\n@@ -219,9 +220,13 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\\n         print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n         start_time = time.time()\\n         nrof_examples = args.people_per_batch * args.images_per_person\\n-        print("nrof_examples =========+>", nrof_examples)\\n+        print("nrof_examples =========+>", nrof_examples , np.arange(nrof_examples))\\n         labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n+        # labels_array = np.arange(nrof_examples)\\n+        print("DONEEEE labels_array=========+>", labels_array)\\n         image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n+        # image_paths_array = np.expand_dims(np.array(image_paths),1)\\n+        print("DONEEEE labels_array=========+>", image_paths_array[0])\\n         sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n         emb_array = np.zeros((nrof_examples, embedding_size))\\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n@@ -361,6 +366,7 @@ def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholde\\n     label_check_array = np.zeros((nrof_images,))\\n     for i in xrange(nrof_batches):\\n         batch_size = min(nrof_images-i*batch_size, batch_size)\\n+        print("Batch size ===========> ", batch_size)\\n         emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\\n             learning_rate_placeholder: 0.0, phase_train_placeholder: False})\\n         emb_array[lab,:] = emb\\n@@ -440,13 +446,13 @@ def parse_arguments(argv):\\n     parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n         help=\\\'Number of epochs to run.\\\', default=50)\\n     parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=1)\\n+        help=\\\'Number of images to process in a batch.\\\', default=11)\\n     parser.add_argument(\\\'--image_size\\\', type=int,\\n         help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n     parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=1)\\n+        help=\\\'Number of people per batch.\\\', default=3)\\n     parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=1)\\n+        help=\\\'Number of images per person.\\\', default=10)\\n     parser.add_argument(\\\'--epoch_size\\\', type=int,\\n         help=\\\'Number of batches per epoch.\\\', default=250)\\n     parser.add_argument(\\\'--alpha\\\', type=float,\'\n\\ No newline at end of file\ndiff --git a/faceNet-yolo/make-classifier.py b/faceNet-yolo/make-classifier.py\nindex 53e59b7..976210b 100644\n--- a/faceNet-yolo/make-classifier.py\n+++ b/faceNet-yolo/make-classifier.py\n@@ -22,7 +22,8 @@ with tf.Graph().as_default():\n         print(\'Number of images: %d\' % len(paths))\n \n         print(\'Loading feature extraction model\')\n-        modeldir = \'./models/facenet/20180402-114759\'\n+        modeldir = \'./models/20170512-110547-working-model\'\n+        print("model dir ===========> ", modeldir)\n         facenet.load_model(modeldir)\n \n         images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\ndiff --git a/faceNet-yolo/models/facenet/20180402-114759/20180402-114759.pb b/faceNet-yolo/models/facenet/20180402-114759/20180402-114759.pb\ndeleted file mode 100644\nindex 39b4ed7..0000000\nBinary files a/faceNet-yolo/models/facenet/20180402-114759/20180402-114759.pb and /dev/null differ\ndiff --git a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.data-00000-of-00001 b/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.data-00000-of-00001\ndeleted file mode 100755\nindex 6160198..0000000\nBinary files a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.data-00000-of-00001 and /dev/null differ\ndiff --git a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.index b/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.index\ndeleted file mode 100755\nindex e2b346c..0000000\nBinary files a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.ckpt-275.index and /dev/null differ\ndiff --git a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.meta b/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.meta\ndeleted file mode 100755\nindex abffaef..0000000\nBinary files a/faceNet-yolo/models/facenet/20180402-114759/model-20180402-114759.meta and /dev/null differ\ndiff --git a/faceNet-yolo/myclassifier/my_classifier.pkl b/faceNet-yolo/myclassifier/my_classifier.pkl\nindex f531d3c..407e8a2 100644\nBinary files a/faceNet-yolo/myclassifier/my_classifier.pkl and b/faceNet-yolo/myclassifier/my_classifier.pkl differ\ndiff --git a/faceNet-yolo/realtime_facenet.py b/faceNet-yolo/realtime_facenet.py\nindex 6d0815b..37a8140 100644\n--- a/faceNet-yolo/realtime_facenet.py\n+++ b/faceNet-yolo/realtime_facenet.py\n@@ -13,6 +13,7 @@ import detect_face\n import os\n import time\n import pickle\n+global str\n \n \n \n@@ -35,7 +36,8 @@ with tf.Graph().as_default():\n         # HumanNames = [\'Andrew\',\'Obama\',\'ZiLin\']    #train human name\n \n         print(\'Loading feature extraction model\')\n-        modeldir = \'./models/facenet/20180402-114759\'\n+        modeldir = \'./models/20170512-110547-new\'  #pre-trained model\n+        # modeldir = \'./models/facenet/20200415-171023\'    #Self-trained model   \n         facenet.load_model(modeldir)\n \n         images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n@@ -57,107 +59,110 @@ with tf.Graph().as_default():\n         #     cv2.imshow(\'IPWebcam\',img)\n         #     if cv2.waitKey(1) == 27:\n         #         break\n-        url=\'http://192.168.43.1:8080/shot.jpg\'\n+        url=\'http://192.168.43.1:8080/video\'\n         video_capture = cv2.VideoCapture(url)\n         c = 0\n \n         print(\'Start Recognition!\')\n         prevTime = 0\n-        \n-        ret, frame = video_capture.read()\n-        print("ret, frame ==================>", ret, frame)\n-        # frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)    #resize frame (optional)\n-\n-        curTime = time.time()    # calc fps\n-        timeF = frame_interval\n-\n-        if (c % timeF == 0):\n-            find_results = []\n-\n-            if frame.ndim == 2:\n-                frame = facenet.to_rgb(frame)\n-            frame = frame[:, :, 0:3]\n-            #print(frame.shape[0])\n-            #print(frame.shape[1])\n-\n-            ## Use MTCNN to get the bounding boxes\n-            bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor)\n-            nrof_faces = bounding_boxes.shape[0]\n-            #print(\'Detected_FaceNum: %d\' % nrof_faces)\n-\n-            if nrof_faces > 0:\n-                det = bounding_boxes[:, 0:4]\n-                img_size = np.asarray(frame.shape)[0:2]\n-\n-                # cropped = []\n-                # scaled = []\n-                # scaled_reshape = []\n-                bb = np.zeros((nrof_faces,4), dtype=np.int32)\n-\n-                for i in range(nrof_faces):\n-                    emb_array = np.zeros((1, embedding_size))\n-\n-                    bb[i][0] = det[i][0]\n-                    bb[i][1] = det[i][1]\n-                    bb[i][2] = det[i][2]\n-                    bb[i][3] = det[i][3]\n-\n-                    if bb[i][0] <= 0 or bb[i][1] <= 0 or bb[i][2] >= len(frame[0]) or bb[i][3] >= len(frame):\n-                        print(\'face is inner of range!\')\n-                        continue\n-\n-                    # cropped.append(frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\n-                    # cropped[0] = facenet.flip(cropped[0], False)\n-                    # scaled.append(misc.imresize(cropped[0], (image_size, image_size), interp=\'bilinear\'))\n-                    # scaled[0] = cv2.resize(scaled[0], (input_image_size,input_image_size),\n-                    #                        interpolation=cv2.INTER_CUBIC)\n-                    # scaled[0] = facenet.prewhiten(scaled[0])\n-                    # scaled_reshape.append(scaled[0].reshape(-1,input_image_size,input_image_size,3))\n-                    # feed_dict = {images_placeholder: scaled_reshape[0], phase_train_placeholder: False}\n-\n-                    cropped = (frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\n-                    print("{0} {1} {2} {3}".format(bb[i][0], bb[i][1], bb[i][2], bb[i][3]))\n-                    cropped = facenet.flip(cropped, False)\n-                    scaled = (misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\'))\n-                    scaled = cv2.resize(scaled, (input_image_size,input_image_size),\n-                                        interpolation=cv2.INTER_CUBIC)\n-                    scaled = facenet.prewhiten(scaled)\n-                    scaled_reshape = (scaled.reshape(-1,input_image_size,input_image_size,3))\n-                    feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-\n-                    emb_array[0, :] = sess.run(embeddings, feed_dict=feed_dict)\n-\n-                    predictions = model.predict_proba(emb_array)\n-                    best_class_indices = np.argmax(predictions, axis=1)\n-                    best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n-\n-                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n-                    text_x = bb[i][0]\n-                    text_y = bb[i][3] + 20\n-                    # for H_i in HumanNames:\n-                    #     if HumanNames[best_class_indices[0]] == H_i:\n-                    result_names = class_names[best_class_indices[0]]\n-                    print("best_class_probabilities ==> ", best_class_probabilities, "best_class_indices ===>", best_class_indices, "result name ===>", result_names , "predictions =====> ", predictions)\n-\n-                    #print(result_names)\n-                    cv2.putText(frame, result_names, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                1, (0, 0, 255), thickness=1, lineType=2)\n-            else:\n-                print(\'Unable to align\')\n-\n-        sec = curTime - prevTime\n-        prevTime = curTime\n-        fps = 1 / (sec)\n-        str = \'FPS: %2.3f\' % fps\n-        text_fps_x = len(frame[0]) - 150\n-        text_fps_y = 20\n-        cv2.putText(frame, str, (text_fps_x, text_fps_y),\n-                    cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 0), thickness=1, lineType=2)\n-        # c+=1\n-        # print("facme =========> ", frame)\n-\n-        cv2.imshow(\'Video\', frame)\n-\n+        while True:\n+            ret, frame = video_capture.read()\n+            # print("frame =================> ", frame)\n+            # print("frame.ndim =================> ", frame.ndim)\n+            frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)    #resize frame (optional)\n+\n+            curTime = time.time()    # calc fps\n+            timeF = frame_interval\n+\n+            if (c % timeF == 0):\n+                find_results = []\n+\n+                if frame.ndim == 2:\n+                    frame = facenet.to_rgb(frame)\n+                frame = frame[:, :, 0:3]\n+                #print(frame.shape[0])\n+                #print(frame.shape[1])\n+\n+                ## Use MTCNN to get the bounding boxes\n+                bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor)\n+                nrof_faces = bounding_boxes.shape[0]\n+                #print(\'Detected_FaceNum: %d\' % nrof_faces)\n+\n+                if nrof_faces > 0:\n+                    det = bounding_boxes[:, 0:4]\n+                    img_size = np.asarray(frame.shape)[0:2]\n+\n+                    # cropped = []\n+                    # scaled = []\n+                    # scaled_reshape = []\n+                    bb = np.zeros((nrof_faces,4), dtype=np.int32)\n+\n+                    for i in range(nrof_faces):\n+                        emb_array = np.zeros((1, embedding_size))\n+\n+                        bb[i][0] = det[i][0]\n+                        bb[i][1] = det[i][1]\n+                        bb[i][2] = det[i][2]\n+                        bb[i][3] = det[i][3]\n+\n+                        if bb[i][0] <= 0 or bb[i][1] <= 0 or bb[i][2] >= len(frame[0]) or bb[i][3] >= len(frame):\n+                            print(\'face is inner of range!\')\n+                            continue\n+\n+                        # cropped.append(frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\n+                        # cropped[0] = facenet.flip(cropped[0], False)\n+                        # scaled.append(misc.imresize(cropped[0], (image_size, image_size), interp=\'bilinear\'))\n+                        # scaled[0] = cv2.resize(scaled[0], (input_image_size,input_image_size),\n+                        #                        interpolation=cv2.INTER_CUBIC)\n+                        # scaled[0] = facenet.prewhiten(scaled[0])\n+                        # scaled_reshape.append(scaled[0].reshape(-1,input_image_size,input_image_size,3))\n+                        # feed_dict = {images_placeholder: scaled_reshape[0], phase_train_placeholder: False}\n+\n+                        cropped = (frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\n+                        print("{0} {1} {2} {3}".format(bb[i][0], bb[i][1], bb[i][2], bb[i][3]))\n+                        cropped = facenet.flip(cropped, False)\n+                        scaled = (misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\'))\n+                        scaled = cv2.resize(scaled, (input_image_size,input_image_size),\n+                                            interpolation=cv2.INTER_CUBIC)\n+                        scaled = facenet.prewhiten(scaled)\n+                        scaled_reshape = (scaled.reshape(-1,input_image_size,input_image_size,3))\n+                        feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n+\n+                        emb_array[0, :] = sess.run(embeddings, feed_dict=feed_dict)\n+\n+                        predictions = model.predict_proba(emb_array)\n+                        best_class_indices = np.argmax(predictions, axis=1)\n+                        best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n+                        cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n+                        text_x = bb[i][0]\n+                        text_y = bb[i][3] + 20\n+\n+                        # for H_i in HumanNames:\n+                        #     if HumanNames[best_class_indices[0]] == H_i:\n+                        result_names = class_names[best_class_indices[0]]\n+                        # strprob =  str(best_class_probabilities[0])\n+                        # result_names = result_names  + strprob\n+                        print("best_class_probabilities ==> ", best_class_probabilities[0], "best_class_indices ===>", best_class_indices, "result name ===>", result_names , "predictions =====> ", predictions)\n+                        #print(result_names)\n+                        cv2.putText(frame, result_names, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n+                                    1, (0, 0, 255), thickness=1, lineType=2)\n+                else:\n+                    print(\'Unable to align\')\n+\n+            sec = curTime - prevTime\n+            prevTime = curTime\n+            fps = 1 / (sec)\n+            str = \'FPS: %2.3f\' % fps\n+            text_fps_x = len(frame[0]) - 150\n+            text_fps_y = 20\n+            cv2.putText(frame, str, (text_fps_x, text_fps_y),\n+                        cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 0), thickness=1, lineType=2)\n+            # c+=1\n+\n+            cv2.imshow(\'Video\', frame)\n+\n+            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n+                break\n \n         video_capture.release()\n         # #video writer\ndiff --git a/faceNet-yolo/train_tripleloss.py b/faceNet-yolo/train_tripleloss.py\nindex 8e2fd93..898de6f 100644\n--- a/faceNet-yolo/train_tripleloss.py\n+++ b/faceNet-yolo/train_tripleloss.py\n@@ -444,9 +444,9 @@ def parse_arguments(argv):\n     parser.add_argument(\'--model_def\', type=str,\n         help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n     parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=50)\n+        help=\'Number of epochs to run.\', default=3)\n     parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=11)\n+        help=\'Number of images to process in a batch.\', default=15)\n     parser.add_argument(\'--image_size\', type=int,\n         help=\'Image size (height, width) in pixels.\', default=160)\n     parser.add_argument(\'--people_per_batch\', type=int,\n@@ -454,7 +454,7 @@ def parse_arguments(argv):\n     parser.add_argument(\'--images_per_person\', type=int,\n         help=\'Number of images per person.\', default=10)\n     parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=250)\n+        help=\'Number of batches per epoch.\', default=10)\n     parser.add_argument(\'--alpha\', type=float,\n         help=\'Positive to negative triplet distance margin.\', default=0.3)\n     parser.add_argument(\'--embedding_size\', type=int,\ndiff --git a/faceNet-yolo/unaligned_faces/s1/pr_train_1.jpg b/faceNet-yolo/unaligned_faces/s1/pr_train_1.jpg\ndeleted file mode 100644\nindex ed8e38a..0000000\nBinary files a/faceNet-yolo/unaligned_faces/s1/pr_train_1.jpg and /dev/null differ\ndiff --git a/faceNet-yolo/unaligned_faces/s2/ishita1.jpg b/faceNet-yolo/unaligned_faces/s2/ishita1.jpg\ndeleted file mode 100644\nindex 014fcb5..0000000\nBinary files a/faceNet-yolo/unaligned_faces/s2/ishita1.jpg and /dev/null differ\ndiff --git a/faceNet-yolo/unaligned_faces/s3/papa2.jpg b/faceNet-yolo/unaligned_faces/s3/papa2.jpg\ndeleted file mode 100644\nindex 7c3ff3a..0000000\nBinary files a/faceNet-yolo/unaligned_faces/s3/papa2.jpg and /dev/null differ'